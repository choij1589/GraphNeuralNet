{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/02\n"
     ]
    }
   ],
   "source": [
    "from ROOT import TFile\n",
    "from EventToGraph import rtfile_to_datalist, MyDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sig = TFile.Open(\n",
    "    \"./SelectorOutput/2017/Skim1E2Mu__/Selector_TTToHcToWA_AToMuMu_MHc130_MA90.root\"\n",
    ")\n",
    "f_bkg = TFile.Open(\n",
    "    \"./SelectorOutput/2017/Skim1E2Mu__/Selector_TTLL_powheg.root\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_datalist = rtfile_to_datalist(f_sig, is_signal=True)\n",
    "bkg_datalist = rtfile_to_datalist(f_bkg, is_signal=False)\n",
    "datalist = shuffle(sig_datalist + bkg_datalist)\n",
    "train_dataset = MyDataset(datalist[:11000])\n",
    "test_dataset = MyDataset(datalist[11000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, BatchNorm1d\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GraphConv, GraphNorm\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "# TODOL: Make EdgeConv layer!\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GNN, self).__init__()\n",
    "        self.graphnorm0 = GraphNorm(train_dataset[0].num_node_features)\n",
    "        self.conv1 = GraphConv(train_dataset[0].num_node_features, 256)\n",
    "        self.graphnorm1 = GraphNorm(256)\n",
    "        self.conv2 = GraphConv(256, 256)\n",
    "        self.graphnorm2 = GraphNorm(256)\n",
    "        self.conv3 = GraphConv(256, 256)\n",
    "        self.fc1 = Linear(256, 256)\n",
    "        self.bn1 = BatchNorm1d(256)\n",
    "        self.fc2 = Linear(256, 2)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embedding\n",
    "        x = self.graphnorm0(x)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.graphnorm1(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.graphnorm2(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.1, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "model = GNN().to(DEVICE)\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=0.02)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    \n",
    "    for data in train_loader:\n",
    "        out = model(data.x.to(DEVICE), data.edge_index.to(DEVICE), data.batch.to(DEVICE))\n",
    "        loss = criterion(out, data.y.to(DEVICE))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        out = model(data.x.to(DEVICE), data.edge_index.to(DEVICE), data.batch.to(DEVICE))\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += int((pred == data.y.to(DEVICE)).sum())\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 0]\tTrain Acc: 0.6411,\tTest Acc: 0.6383\n",
      "[EPOCH 1]\tTrain Acc: 0.6865,\tTest Acc: 0.6877\n",
      "[EPOCH 2]\tTrain Acc: 0.6999,\tTest Acc: 0.6988\n",
      "[EPOCH 3]\tTrain Acc: 0.7070,\tTest Acc: 0.7072\n",
      "[EPOCH 4]\tTrain Acc: 0.7134,\tTest Acc: 0.7126\n",
      "[EPOCH 5]\tTrain Acc: 0.7196,\tTest Acc: 0.7176\n",
      "[EPOCH 6]\tTrain Acc: 0.7231,\tTest Acc: 0.7208\n",
      "[EPOCH 7]\tTrain Acc: 0.7271,\tTest Acc: 0.7257\n",
      "[EPOCH 8]\tTrain Acc: 0.7300,\tTest Acc: 0.7308\n",
      "[EPOCH 9]\tTrain Acc: 0.7351,\tTest Acc: 0.7343\n",
      "[EPOCH 10]\tTrain Acc: 0.7380,\tTest Acc: 0.7367\n",
      "[EPOCH 11]\tTrain Acc: 0.7395,\tTest Acc: 0.7398\n",
      "[EPOCH 12]\tTrain Acc: 0.7403,\tTest Acc: 0.7413\n",
      "[EPOCH 13]\tTrain Acc: 0.7428,\tTest Acc: 0.7423\n",
      "[EPOCH 14]\tTrain Acc: 0.7423,\tTest Acc: 0.7432\n",
      "[EPOCH 15]\tTrain Acc: 0.7439,\tTest Acc: 0.7443\n",
      "[EPOCH 16]\tTrain Acc: 0.7443,\tTest Acc: 0.7443\n",
      "[EPOCH 17]\tTrain Acc: 0.7469,\tTest Acc: 0.7440\n",
      "[EPOCH 18]\tTrain Acc: 0.7434,\tTest Acc: 0.7454\n",
      "[EPOCH 19]\tTrain Acc: 0.7466,\tTest Acc: 0.7457\n",
      "[EPOCH 20]\tTrain Acc: 0.7474,\tTest Acc: 0.7456\n",
      "[EPOCH 21]\tTrain Acc: 0.7481,\tTest Acc: 0.7452\n",
      "[EPOCH 22]\tTrain Acc: 0.7482,\tTest Acc: 0.7455\n",
      "[EPOCH 23]\tTrain Acc: 0.7487,\tTest Acc: 0.7465\n",
      "[EPOCH 24]\tTrain Acc: 0.7508,\tTest Acc: 0.7467\n",
      "[EPOCH 25]\tTrain Acc: 0.7498,\tTest Acc: 0.7465\n",
      "[EPOCH 26]\tTrain Acc: 0.7515,\tTest Acc: 0.7461\n",
      "[EPOCH 27]\tTrain Acc: 0.7486,\tTest Acc: 0.7466\n",
      "[EPOCH 28]\tTrain Acc: 0.7525,\tTest Acc: 0.7469\n",
      "[EPOCH 29]\tTrain Acc: 0.7525,\tTest Acc: 0.7462\n",
      "[EPOCH 30]\tTrain Acc: 0.7508,\tTest Acc: 0.7470\n",
      "[EPOCH 31]\tTrain Acc: 0.7533,\tTest Acc: 0.7463\n",
      "[EPOCH 32]\tTrain Acc: 0.7530,\tTest Acc: 0.7474\n",
      "[EPOCH 33]\tTrain Acc: 0.7544,\tTest Acc: 0.7479\n",
      "[EPOCH 34]\tTrain Acc: 0.7535,\tTest Acc: 0.7488\n",
      "[EPOCH 35]\tTrain Acc: 0.7537,\tTest Acc: 0.7486\n",
      "[EPOCH 36]\tTrain Acc: 0.7545,\tTest Acc: 0.7481\n",
      "[EPOCH 37]\tTrain Acc: 0.7545,\tTest Acc: 0.7486\n",
      "[EPOCH 38]\tTrain Acc: 0.7565,\tTest Acc: 0.7482\n",
      "[EPOCH 39]\tTrain Acc: 0.7543,\tTest Acc: 0.7493\n",
      "[EPOCH 40]\tTrain Acc: 0.7571,\tTest Acc: 0.7484\n",
      "[EPOCH 41]\tTrain Acc: 0.7574,\tTest Acc: 0.7491\n",
      "[EPOCH 42]\tTrain Acc: 0.7573,\tTest Acc: 0.7491\n",
      "[EPOCH 43]\tTrain Acc: 0.7559,\tTest Acc: 0.7496\n",
      "[EPOCH 44]\tTrain Acc: 0.7585,\tTest Acc: 0.7498\n",
      "[EPOCH 45]\tTrain Acc: 0.7565,\tTest Acc: 0.7490\n",
      "[EPOCH 46]\tTrain Acc: 0.7578,\tTest Acc: 0.7490\n",
      "[EPOCH 47]\tTrain Acc: 0.7595,\tTest Acc: 0.7485\n",
      "[EPOCH 48]\tTrain Acc: 0.7577,\tTest Acc: 0.7488\n",
      "[EPOCH 49]\tTrain Acc: 0.7591,\tTest Acc: 0.7499\n",
      "[EPOCH 50]\tTrain Acc: 0.7603,\tTest Acc: 0.7488\n",
      "[EPOCH 51]\tTrain Acc: 0.7590,\tTest Acc: 0.7482\n",
      "[EPOCH 52]\tTrain Acc: 0.7599,\tTest Acc: 0.7492\n",
      "[EPOCH 53]\tTrain Acc: 0.7593,\tTest Acc: 0.7488\n",
      "[EPOCH 54]\tTrain Acc: 0.7610,\tTest Acc: 0.7483\n",
      "[EPOCH 55]\tTrain Acc: 0.7605,\tTest Acc: 0.7474\n",
      "[EPOCH 56]\tTrain Acc: 0.7616,\tTest Acc: 0.7486\n",
      "[EPOCH 57]\tTrain Acc: 0.7585,\tTest Acc: 0.7483\n",
      "[EPOCH 58]\tTrain Acc: 0.7616,\tTest Acc: 0.7473\n",
      "[EPOCH 59]\tTrain Acc: 0.7588,\tTest Acc: 0.7465\n",
      "[EPOCH 60]\tTrain Acc: 0.7619,\tTest Acc: 0.7474\n",
      "[EPOCH 61]\tTrain Acc: 0.7657,\tTest Acc: 0.7470\n",
      "[EPOCH 62]\tTrain Acc: 0.7625,\tTest Acc: 0.7466\n",
      "[EPOCH 63]\tTrain Acc: 0.7628,\tTest Acc: 0.7473\n",
      "[EPOCH 64]\tTrain Acc: 0.7631,\tTest Acc: 0.7478\n",
      "[EPOCH 65]\tTrain Acc: 0.7638,\tTest Acc: 0.7471\n",
      "[EPOCH 66]\tTrain Acc: 0.7639,\tTest Acc: 0.7479\n",
      "[EPOCH 67]\tTrain Acc: 0.7660,\tTest Acc: 0.7466\n",
      "[EPOCH 68]\tTrain Acc: 0.7615,\tTest Acc: 0.7477\n",
      "[EPOCH 69]\tTrain Acc: 0.7630,\tTest Acc: 0.7467\n",
      "[EPOCH 70]\tTrain Acc: 0.7636,\tTest Acc: 0.7473\n",
      "[EPOCH 71]\tTrain Acc: 0.7637,\tTest Acc: 0.7464\n",
      "[EPOCH 72]\tTrain Acc: 0.7632,\tTest Acc: 0.7475\n",
      "[EPOCH 73]\tTrain Acc: 0.7634,\tTest Acc: 0.7469\n",
      "[EPOCH 74]\tTrain Acc: 0.7667,\tTest Acc: 0.7452\n",
      "[EPOCH 75]\tTrain Acc: 0.7666,\tTest Acc: 0.7472\n",
      "[EPOCH 76]\tTrain Acc: 0.7645,\tTest Acc: 0.7467\n",
      "[EPOCH 77]\tTrain Acc: 0.7654,\tTest Acc: 0.7468\n",
      "[EPOCH 78]\tTrain Acc: 0.7655,\tTest Acc: 0.7469\n",
      "[EPOCH 79]\tTrain Acc: 0.7629,\tTest Acc: 0.7483\n",
      "[EPOCH 80]\tTrain Acc: 0.7675,\tTest Acc: 0.7469\n",
      "[EPOCH 81]\tTrain Acc: 0.7649,\tTest Acc: 0.7464\n",
      "[EPOCH 82]\tTrain Acc: 0.7672,\tTest Acc: 0.7472\n",
      "[EPOCH 83]\tTrain Acc: 0.7642,\tTest Acc: 0.7472\n",
      "[EPOCH 84]\tTrain Acc: 0.7671,\tTest Acc: 0.7472\n",
      "[EPOCH 85]\tTrain Acc: 0.7667,\tTest Acc: 0.7459\n",
      "[EPOCH 86]\tTrain Acc: 0.7679,\tTest Acc: 0.7473\n",
      "[EPOCH 87]\tTrain Acc: 0.7666,\tTest Acc: 0.7469\n",
      "[EPOCH 88]\tTrain Acc: 0.7668,\tTest Acc: 0.7463\n",
      "[EPOCH 89]\tTrain Acc: 0.7674,\tTest Acc: 0.7453\n",
      "[EPOCH 90]\tTrain Acc: 0.7667,\tTest Acc: 0.7467\n",
      "[EPOCH 91]\tTrain Acc: 0.7680,\tTest Acc: 0.7462\n",
      "[EPOCH 92]\tTrain Acc: 0.7675,\tTest Acc: 0.7465\n",
      "[EPOCH 93]\tTrain Acc: 0.7664,\tTest Acc: 0.7469\n",
      "[EPOCH 94]\tTrain Acc: 0.7668,\tTest Acc: 0.7457\n",
      "[EPOCH 95]\tTrain Acc: 0.7695,\tTest Acc: 0.7461\n",
      "[EPOCH 96]\tTrain Acc: 0.7668,\tTest Acc: 0.7461\n",
      "[EPOCH 97]\tTrain Acc: 0.7690,\tTest Acc: 0.7467\n",
      "[EPOCH 98]\tTrain Acc: 0.7685,\tTest Acc: 0.7459\n",
      "[EPOCH 99]\tTrain Acc: 0.7692,\tTest Acc: 0.7472\n",
      "[EPOCH 100]\tTrain Acc: 0.7696,\tTest Acc: 0.7459\n",
      "[EPOCH 101]\tTrain Acc: 0.7686,\tTest Acc: 0.7471\n",
      "[EPOCH 102]\tTrain Acc: 0.7670,\tTest Acc: 0.7466\n",
      "[EPOCH 103]\tTrain Acc: 0.7683,\tTest Acc: 0.7476\n",
      "[EPOCH 104]\tTrain Acc: 0.7698,\tTest Acc: 0.7457\n",
      "[EPOCH 105]\tTrain Acc: 0.7696,\tTest Acc: 0.7467\n",
      "[EPOCH 106]\tTrain Acc: 0.7693,\tTest Acc: 0.7452\n",
      "[EPOCH 107]\tTrain Acc: 0.7687,\tTest Acc: 0.7456\n",
      "[EPOCH 108]\tTrain Acc: 0.7708,\tTest Acc: 0.7476\n",
      "[EPOCH 109]\tTrain Acc: 0.7688,\tTest Acc: 0.7458\n",
      "[EPOCH 110]\tTrain Acc: 0.7704,\tTest Acc: 0.7458\n",
      "[EPOCH 111]\tTrain Acc: 0.7724,\tTest Acc: 0.7463\n",
      "[EPOCH 112]\tTrain Acc: 0.7721,\tTest Acc: 0.7471\n",
      "[EPOCH 113]\tTrain Acc: 0.7710,\tTest Acc: 0.7457\n",
      "[EPOCH 114]\tTrain Acc: 0.7724,\tTest Acc: 0.7469\n",
      "[EPOCH 115]\tTrain Acc: 0.7719,\tTest Acc: 0.7466\n",
      "[EPOCH 116]\tTrain Acc: 0.7709,\tTest Acc: 0.7465\n",
      "[EPOCH 117]\tTrain Acc: 0.7722,\tTest Acc: 0.7459\n",
      "[EPOCH 118]\tTrain Acc: 0.7713,\tTest Acc: 0.7462\n",
      "[EPOCH 119]\tTrain Acc: 0.7702,\tTest Acc: 0.7469\n",
      "[EPOCH 120]\tTrain Acc: 0.7723,\tTest Acc: 0.7469\n",
      "[EPOCH 121]\tTrain Acc: 0.7728,\tTest Acc: 0.7467\n",
      "[EPOCH 122]\tTrain Acc: 0.7724,\tTest Acc: 0.7460\n",
      "[EPOCH 123]\tTrain Acc: 0.7753,\tTest Acc: 0.7457\n",
      "[EPOCH 124]\tTrain Acc: 0.7731,\tTest Acc: 0.7475\n",
      "[EPOCH 125]\tTrain Acc: 0.7731,\tTest Acc: 0.7471\n",
      "[EPOCH 126]\tTrain Acc: 0.7718,\tTest Acc: 0.7463\n",
      "[EPOCH 127]\tTrain Acc: 0.7726,\tTest Acc: 0.7459\n",
      "[EPOCH 128]\tTrain Acc: 0.7739,\tTest Acc: 0.7481\n",
      "[EPOCH 129]\tTrain Acc: 0.7750,\tTest Acc: 0.7470\n",
      "[EPOCH 130]\tTrain Acc: 0.7736,\tTest Acc: 0.7473\n",
      "[EPOCH 131]\tTrain Acc: 0.7752,\tTest Acc: 0.7459\n",
      "[EPOCH 132]\tTrain Acc: 0.7739,\tTest Acc: 0.7467\n",
      "[EPOCH 133]\tTrain Acc: 0.7762,\tTest Acc: 0.7473\n",
      "[EPOCH 134]\tTrain Acc: 0.7715,\tTest Acc: 0.7485\n",
      "[EPOCH 135]\tTrain Acc: 0.7745,\tTest Acc: 0.7475\n",
      "[EPOCH 136]\tTrain Acc: 0.7768,\tTest Acc: 0.7468\n",
      "[EPOCH 137]\tTrain Acc: 0.7751,\tTest Acc: 0.7469\n",
      "[EPOCH 138]\tTrain Acc: 0.7755,\tTest Acc: 0.7469\n",
      "[EPOCH 139]\tTrain Acc: 0.7767,\tTest Acc: 0.7474\n",
      "[EPOCH 140]\tTrain Acc: 0.7726,\tTest Acc: 0.7478\n",
      "[EPOCH 141]\tTrain Acc: 0.7766,\tTest Acc: 0.7459\n",
      "[EPOCH 142]\tTrain Acc: 0.7775,\tTest Acc: 0.7466\n",
      "[EPOCH 143]\tTrain Acc: 0.7758,\tTest Acc: 0.7459\n",
      "[EPOCH 144]\tTrain Acc: 0.7781,\tTest Acc: 0.7458\n",
      "[EPOCH 145]\tTrain Acc: 0.7765,\tTest Acc: 0.7473\n",
      "[EPOCH 146]\tTrain Acc: 0.7737,\tTest Acc: 0.7473\n",
      "[EPOCH 147]\tTrain Acc: 0.7779,\tTest Acc: 0.7459\n",
      "[EPOCH 148]\tTrain Acc: 0.7786,\tTest Acc: 0.7471\n",
      "[EPOCH 149]\tTrain Acc: 0.7770,\tTest Acc: 0.7478\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(150):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'[EPOCH {epoch}]\\tTrain Acc: {train_acc:.4f},\\tTest Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f64be793538f7fe230f350828c9baf03d97c4df0981f52e8388f53f367f4a42"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
